{
 "metadata": {
  "name": "",
  "signature": "sha256:13271eaa44bbaafa45fae786bbfc13c6e3ee9cf722bea315484f2d1d224cab1f"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Initial Code for PIHM hydrologic model parallelization\n",
      "\n",
      "###Proposed MPI PIHM hydrologic model structure\n",
      "\n",
      "| File              |  Content                        |  Type    |\n",
      "|-------------------|---------------------------------|----------|\n",
      "| pihm.h            | defining global variables       | global   |\n",
      "| pihm.c            | main function                   | global   |\n",
      "| read_alloc.c      | read in all the input files     | local    |\n",
      "| initialize.c      | initialization states and flux  | local    |\n",
      "| f.c               | states and flux calculation     | local    |\n",
      "| f_functions.h     | functions used in f.c           | local    |\n",
      "| print.c           | print out the outputs           | global   |\n",
      "| print_functions.h | functions used in print.c       | global   | \n",
      "\n",
      "All the local machines reads the input files and initialize the states and fluxes into MD->DummyY, then assign these values to local Y, then do calculations on the local Y.\n",
      "At each time step, CVODE will obtain the errors (y_dot vs. [y(t+deltaT)-y(t)]/deltaT) for all the processrors and obtain the largest error among all the processors. If the largest error is less than the tolerance, then move ahead, otherwise, initiates another guess and compare the errors.\n",
      "\n",
      "###Outline for the work\n",
      "\n",
      "Since the current PIHM version have all the land element (triangular) and river element (rectangular) defined in a global data frame, and in order to map those elements into different processors, one need to come up with the way to optimize the MPI efficiency:\n",
      "\n",
      "* Load-Balance: Effeiciently divide the watershed into \"equal\" subwatershed\n",
      "    * number of elements in each subwatershed should be as close as possible\n",
      "    * Build a lookup table for each processor and element ID\n",
      "    \n",
      "* Minimize Communication: checking convergence for the whole watershed at each time step\n",
      "    * Copy global value into local value (reduce the frequency of calling global variable)\n",
      "    * Checking convergence criteria after certain time step ? or one time step? (need to think)\n",
      "\n",
      "\n",
      "###Initial code component (adding on the old series code)\n",
      "* Relation of local grids and global elements has been build\n",
      "    * i = locI + j*NumEle\n",
      "* Communications of processors has been build\n",
      "    * MPI send\n",
      "    * MPI receive\n",
      "* Calculations of stages and fluxes\n",
      "    * modifing the functions to make it calculate with available local values\n",
      "\n",
      "####To be done:\n",
      "* Results of series code and MPI code will be compared for a simple 2 cell case\n",
      "    * run with different processors using the same code\n",
      "* Results of series code and MPI code will be compared for a real watershed\n",
      "    * Lake Michie watershed\n",
      "* Optimization of the data partition will be done\n",
      "    * Graph theory to partition the grids"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}